## Домашнее задание ## 

Разворачиваем и настраиваем БД с большими данными  
Цель: - знать различные механизмы загрузки данных  
- уметь пользоваться различными механизмами загрузки данных  


Необходимо провести сравнение скорости работы  запросов на различных СУБД   

### 1) Выбрать одну из СУБД ###

**Percona Server for MySQL**  

https://www.percona.com/doc/percona-server/5.7/installation/apt_repo.html  
 sudo apt-get install gnupg2  
 wget https://repo.percona.com/apt/percona-release_latest.$(lsb_release -sc)_all.deb  
 sudo dpkg -i percona-release_latest.$(lsb_release -sc)_all.deb  
 sudo apt-get update  
 sudo apt-get install percona-server-server-5.7  
 service mysql status  
 

### 2) Загрузить в неё данные (10 Гб) ###

есть файл с данными по музыкальным произведеним на 16 млн. строк (3.2Gb )  
залью просто три раза его :) 
с рандомным признаком и уникальным id  

```
 mysql -u root -p 
create database test012;
use test012;
create table muzik(
 id int not null primary key auto_increment,
name_orig text,
performers text ,
publisher text,
author_music text,
author_text text,
album_name text,
duration text,
public_year text,
isrc text,
date_in timestamp default now(),
random_key int
);
```

Заполняем данными     

команда   
 
LOAD DATA INFILE '/data/tmp/meta_data.csv' IGNORE  
INTO TABLE muzik  
FIELDS TERMINATED BY ';' ENCLOSED BY '"'  
LINES TERMINATED BY '\n'  
IGNORE 1 ROWS  
(name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc);  

проверяем размер :   

SELECT  
table_name AS `muzik`,  
round(((data_length + index_length) / 1024/1024), 2) `Size in MB`  
FROM information_schema.TABLES  
WHERE table_schema = "test012";  

выполнение : 

```
 LOAD DATA INFILE '/data/tmp/meta_data.csv' IGNORE
    -> INTO TABLE muzik
    -> FIELDS TERMINATED BY ';' ENCLOSED BY '"'
    -> LINES TERMINATED BY '\n'
    -> IGNORE 1 ROWS
    -> (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc);
Query OK, 16632064 rows affected, 6 warnings (2 min 12.56 sec)
Records: 16632064  Deleted: 0  Skipped: 0  Warnings: 6

mysql> SELECT
    -> table_name AS `muzik`,
    -> round(((data_length + index_length) / 1024/1024), 2) `Size in MB`
    -> FROM information_schema.TABLES
    -> WHERE table_schema = "test012";
+-------+------------+
| muzik | Size in MB |
+-------+------------+
| muzik |    3528.00 |
+-------+------------+
1 row in set (0.01 sec)

```

3.5 гигов. а надо 10. просто залил еще 2 раза тож самое :) .   

```
mysql> LOAD DATA INFILE '/data/tmp/meta_data.csv' IGNORE
    -> INTO TABLE muzik
    -> FIELDS TERMINATED BY ';' ENCLOSED BY '"'
    -> LINES TERMINATED BY '\n'
    -> IGNORE 1 ROWS
    -> (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc);
Query OK, 16632064 rows affected, 6 warnings (2 min 13.06 sec)
Records: 16632064  Deleted: 0  Skipped: 0  Warnings: 6

mysql> LOAD DATA INFILE '/data/tmp/meta_data.csv' IGNORE INTO TABLE muzik FIELDS TERMINATED BY ';' ENCLOSED BY '"' LINES TERMINATED BY '\n' IGNORE 1 ROWS (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc);
Query OK, 16632064 rows affected, 6 warnings (2 min 12.90 sec)
Records: 16632064  Deleted: 0  Skipped: 0  Warnings: 6

mysql> SELECT
    -> table_name AS `muzik`,
    -> round(((data_length + index_length) / 1024/1024), 2) `Size in MB`
    -> FROM information_schema.TABLES
    -> WHERE table_schema = "test012";
+-------+------------+
| muzik | Size in MB |
+-------+------------+
| muzik |   11099.00 |
+-------+------------+
1 row in set (0.00 sec)

 select count(1) from muzik;
+----------+
| count(1) |
+----------+
| 49896192 |
+----------+
1 row in set (13.90 sec)


-- строим индексы

mysql> alter table muzik add index p(performers) ;
Query OK, 0 rows affected (2 min 56.91 sec)
Records: 0  Duplicates: 0  Warnings: 0


mysql> alter table muzik add index ( name_orig) ;
Query OK, 0 rows affected (3 min 1.58 sec)
Records: 0  Duplicates: 0  Warnings: 0

```

**11 гигов.**  
задание выполнено :)   

### 3) Сравнить скорость выполнения запросов на PosgreSQL и выбранной СУБД ###

получил постгерс :   
https://www.postgresql.org/download/linux/debian/   

\# Create the file repository configuration:  
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'  

\# Import the repository signing key:  
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -  

\# Update the package lists:  
sudo apt-get update  

\# Install the latest version of PostgreSQL.  
\# If you want a specific version, use 'postgresql-12' or similar instead of 'postgresql':  
sudo apt-get -y install postgresql  

создаем такую же базу.   
и заполняем ее теми же данными   

```
sudo -u postgres psql
create database test012;
\c test012;
create table muzik(
 id bigserial,
name_orig text,
performers text ,
publisher text,
author_music text,
author_text text,
album_name text,
duration text,
public_year text,
isrc text,
date_in timestamp default now(),
random_key int
);
```

команда :   
cat /data/tmp/meta_data.csv | sudo -u postgres psql -d test012  -c "COPY muzik  (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc) FROM STDIN DELIMITER ';' quote '\"' escape '\"'  CSV HEADER"   

выполняем  3 раза. 

```
 cat /data/tmp/meta_data.csv | sudo -u postgres psql -d test012  -c "COPY muzik  (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc) FROM STDIN DELIMITER ';' quote '\"' escape '\"'  CSV HEADER" 
COPY 16632064

cat /data/tmp/meta_data.csv | sudo -u postgres psql -d test012  -c "COPY muzik  (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc) FROM STDIN DELIMITER ';' quote '\"' escape '\"'  CSV HEADER" 
COPY 16632064

cat /data/tmp/meta_data.csv | sudo -u postgres psql -d test012  -c "COPY muzik  (name_orig,performers,publisher,author_music,author_text,album_name,duration,public_year,isrc) FROM STDIN DELIMITER ';' quote '\"' escape '\"'  CSV HEADER" 
COPY 16632064

sudo -u postgres psql


test012=# \dt+
                           List of relations
 Schema | Name  | Type  |  Owner   | Persistence | Size  | Description 
--------+-------+-------+----------+-------------+-------+-------------
 public | muzik | table | postgres | permanent   | 12 GB | 
(1 row)

 select count(1) from muzik;
  count   
----------
 49896192
(1 row)

```
тоже создаем индекс чтоб одинаково все было :   

CREATE  INDEX muzik_performers_name_orig ON muzik USING btree (performers, name_orig);  

начинаем тестировать :  

**Postgresql**  

```
-- ищем произвольного исполнителя 

 explain analyze select * from muzik where performers ='Lan Ge';
                                                                QUERY PLAN                                                                
------------------------------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on muzik  (cost=27.47..7537.19 rows=1923 width=214) (actual time=0.185..1.272 rows=1014 loops=1)
   Recheck Cond: (performers = 'Lan Ge'::text)
   Heap Blocks: exact=739
   ->  Bitmap Index Scan on muzik_performers_name_orig  (cost=0.00..26.99 rows=1923 width=0) (actual time=0.101..0.101 rows=1014 loops=1)
         Index Cond: (performers = 'Lan Ge'::text)
 Planning Time: 0.103 ms
 Execution Time: 1.379 ms
(7 rows)

-- по полному индексу : 

 select count(1) from muzik where performers ='Lan Ge'  and name_orig='Wo Ai Qia Qia';
 count 
-------
     6
(1 row)

explain analyze select * from muzik where performers ='Lan Ge'  and name_orig='Wo Ai Qia Qia';
                                                             QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
 Index Scan using muzik_performers_name_orig on muzik  (cost=0.56..8.58 rows=1 width=214) (actual time=0.046..0.055 rows=6 loops=1)
   Index Cond: ((performers = 'Lan Ge'::text) AND (name_orig = 'Wo Ai Qia Qia'::text))
 Planning Time: 0.119 ms
 Execution Time: 0.074 ms
(4 rows)

```

**MySQL**  
```
 set profiling=1;
  select * from muzik where performers ='Lan Ge'  and name_orig='Wo Ai Qia Qia';
select * from muzik where performers ='Lan Ge';
 show profiles;
+----------+------------+-------------------------------------------------------------------------------+
| Query_ID | Duration   | Query                                                                         |
+----------+------------+-------------------------------------------------------------------------------+
|        1 | 0.00064575 | select * from muzik where performers ='Lan Ge'  and name_orig='Wo Ai Qia Qia' |
|        2 | 0.01470300 | select * from muzik where performers ='Lan Ge'                                |
+----------+------------+-------------------------------------------------------------------------------+
2 rows in set, 1 warning (0.00 sec)

```
Сравниваем с постгри :   
select * from muzik where performers ='Lan Ge'    
  1,47 ms (Mysql)  > 1.37 ms (Postgres)  

 select * from muzik where performers ='Lan Ge'  and name_orig='Wo Ai Qia Qia'   
  0.00064 ms (Mysql)  < 0.074 ms (Postgres)   

Мы видим что там где данных надо отдать больше данных мускль слегка тормозит. 
А там где меньше - уверенно обгоняет. 

Я провел еще несколько тестов на другие слова. показатели примерно такие же.   


### 4) Описать что и как делали и с какими проблемами столкнулись ### 

- Устанавливал базу mysql и  postgeres с настройками по умолчанию.   
- заливал полностью одинаковые данные. (в сумме 11 гигов и  49 млн. записей ) на одном и том сервере - на одном и том же диске. (индексы и запросы строил по очереди. чтоб не пересекались)  
- сделал произвольные запросы к обоим базам настроенным по дефолту .  


И Хотя мускль не расчитан на большие таблици, все же он силен до 15 миллионов строк больше. а дальше шардинг. Но в целом на запросах по индексу он показал лучше время там где нужно было отдать меньше данных. Однако постгри показа себя лучше в плане : заполнения (проще) , построения индексов (мускль не смог построить по text и поля для индекса надо было переделать в varchar)  

**Проблемы**  
в mysql :   
 - проблема —secure-file-priv  
 поправил файл : 
  /etc/mysql/percona-server.conf.d/mysqld.cnf  
  добавил : 
secure-file-priv = "/data/tmp"
т.е. директорию откуда загружаю данные. 
перезапустил mysql - все ок. 
так же надо добавлять IGNORE при LOAD DATA т.к. данные неизветсные. и не все поля заполнены могут быть и мускль ругается. лучше трочку пропустить чем выискивать в какой миллионой строке пропущено поле. 
 
 почему то в постгре таких проблем не возникло при этом количество полей вставилось одинаково  
 Вообще на удивление с постгри прошло все быстрее и проще. Хотя с мусклем я работал гораздо дольше чем изучал постгрю.   
 пожалуй это самое удивительное для меня открытие в этом задании :)   
 
 
 